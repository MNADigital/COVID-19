{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date, datetime, timedelta\n",
    "import untangle # xml\n",
    "import requests # json\n",
    "import re # regular expressions\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "today = date.today()\n",
    "yesterday = date.today() - timedelta(days=1)\n",
    "\n",
    "our_utla_codes = [\n",
    "    'E08000025', # Birmingham\n",
    "    'E08000027', # Dudley\n",
    "    'E08000028', # Sandwell\n",
    "    'E06000051', # Shropshire\n",
    "    'E10000028', # Staffordshire\n",
    "    'E06000021', # Stoke-on-Trent\n",
    "    'E06000020', # Telford and Wrekin\n",
    "    'E08000030', # Walsall\n",
    "    'E08000031', # Wolverhampton\n",
    "    'E10000034', # Worcestershire\n",
    "]\n",
    "utlacodes_expressandstar = [\n",
    "    'E08000025', # Birmingham\n",
    "    'E08000027', # Dudley\n",
    "    'E08000028', # Sandwell\n",
    "    'E10000028', # Staffordshire\n",
    "    'E06000021', # Stoke-on-Trent\n",
    "    'E08000030', # Walsall\n",
    "    'E08000031', # Wolverhampton\n",
    "]\n",
    "utlacodes_shropshirestar = [\n",
    "    'E06000051', # Shropshire\n",
    "    'E06000020', # Telford and Wrekin\n",
    "]\n",
    "utlacodes_blackcountry = [\n",
    "    'E08000027', # Dudley\n",
    "    'E08000028', # Sandwell\n",
    "    'E08000030', # Walsall\n",
    "    'E08000031', # Wolverhampton\n",
    "]\n",
    "utlacodes_blackcountryandbirmingham = [\n",
    "    'E08000025', # Birmingham\n",
    "    'E08000027', # Dudley\n",
    "    'E08000028', # Sandwell\n",
    "    'E08000030', # Walsall\n",
    "    'E08000031', # Wolverhampton\n",
    "]\n",
    "utlacodes_sandwellandbirmingham = [\n",
    "    'E08000025', # Birmingham\n",
    "    'E08000028', # Sandwell\n",
    "]\n",
    "utlacodes_staffordshire = [\n",
    "    'E10000028', # Staffordshire\n",
    "]\n",
    "utlacodes_wolverhampton = [\n",
    "    'E08000031', # Wolverhampton\n",
    "]\n",
    "utlacodes_walsall = [\n",
    "    'E08000030', # Walsall\n",
    "]\n",
    "utlacodes_dudley = [\n",
    "    'E08000027', # Dudley\n",
    "]\n",
    "utlacodes_sandwell = [\n",
    "    'E08000028', # Sandwell\n",
    "]\n",
    "utlacodes_birmingham = [\n",
    "    'E08000025', # Birmingham\n",
    "]\n",
    "utlacodes_stoke = [\n",
    "    'E06000021', # Stoke-on-Trent\n",
    "]\n",
    "utlacodes_staffordshireandstoke = [\n",
    "    'E10000028', # Staffordshire\n",
    "    'E06000021', # Stoke-on-Trent\n",
    "]\n",
    "utlacodes_worcestershire = [\n",
    "    'E10000034', # Worcestershire\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the XML and traverse to the blobs\n",
    "xmlblobs = untangle.parse(\n",
    "    'https://publicdashacc.blob.core.windows.net/publicdata?restype=container&comp=list'\n",
    ").EnumerationResults.Blobs.Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the cdata of the names, filter to pattern, map to an object while extracting the date, de-duplicate on most recent ( due to sort )\n",
    "def reduce_deduplicate(persist,temp):\n",
    "    persist.update(\n",
    "        {\n",
    "            temp['date'].strftime('%Y%m%d') : temp\n",
    "        }\n",
    "    )\n",
    "    return persist\n",
    "\n",
    "pattern = '^data_([0-9]{4}(0[0-9]|1[0-2])([0-2][0-9]|3[0-1])).+\\.json$'\n",
    "datafiles = reduce(\n",
    "    reduce_deduplicate,\n",
    "    map(\n",
    "        lambda filename: {\n",
    "            'date': datetime.strptime(\n",
    "                re.search(pattern,filename).group(1),\n",
    "                '%Y%m%d'\n",
    "            ),\n",
    "            'filename': filename\n",
    "        },\n",
    "        sorted(\n",
    "            filter(\n",
    "                lambda filename: re.search(pattern,filename),\n",
    "                map(\n",
    "                    lambda blob: blob.Name.cdata,\n",
    "                    xmlblobs\n",
    "                )\n",
    "            ),\n",
    "            reverse=False\n",
    "        )\n",
    "    ),\n",
    "    {}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utla_day_to_dataframe(xmljsonref):\n",
    "    xmljsonref = xmljsonref.copy()\n",
    "    # build a url to retrieve\n",
    "    url = 'https://c19pub.azureedge.net/' + xmljsonref['filename']\n",
    "    \n",
    "    # request a URL\n",
    "    json = requests.get(url).json()\n",
    "    \n",
    "    # initialise array\n",
    "    utla_data = []\n",
    "    \n",
    "    # iterate through utlas element of JSON\n",
    "    for (code, data) in json['utlas'].items():\n",
    "        # if the code is one of our UTLAs\n",
    "        if code in our_utla_codes:\n",
    "            # loop through the days of data in the dailyConfirmedCases element\n",
    "            for day in data['dailyConfirmedCases']:\n",
    "                # append a mix of this data and that above\n",
    "                utla_data.append([\n",
    "                    code, # UTLA code\n",
    "                    data['name']['value'], # filename\n",
    "                    day['date'], # date of cases\n",
    "                    day['value'] # cases\n",
    "                ])\n",
    "    xmljsonref['dataframe'] = pd.DataFrame(data=utla_data, columns=['code','name','date of case','cases'])\n",
    "    \n",
    "    return xmljsonref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_add_reported_day(x):\n",
    "    x['dataframe']['date reported'] = x['date']\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_utlas_historical = reduce(\n",
    "    lambda all_data, day_data : all_data.append(day_data,ignore_index=True),\n",
    "    map(\n",
    "        lambda x : x['dataframe'],\n",
    "        map(\n",
    "            map_add_reported_day,\n",
    "            map(\n",
    "                utla_day_to_dataframe,\n",
    "                list(\n",
    "                    datafiles.values()\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    pd.DataFrame(columns = ['code','name','date of case','date reported','cases'])\n",
    ")\n",
    "our_utlas_historical['cases'] = our_utlas_historical['cases'].convert_dtypes(convert_integer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from get_latest_deaths, which really ought to be very similar as it's non-cumulative\n",
    "def get_latest_cases(code_group=False, df = our_utlas_historical.copy()):\n",
    "    # get the last and penultimate reporting dates\n",
    "    last_day = df['date reported'].max()\n",
    "    penultimate_day = last_day - timedelta(days=1)\n",
    "\n",
    "    # remove all but the last two days of data\n",
    "    df = df[(df['date reported'] == last_day) | (df['date reported'] == penultimate_day)]\n",
    "    \n",
    "    # group\n",
    "    df = df.sort_values(by=['date reported'], ascending=False)\n",
    "    df = df.groupby(['code','name','date reported'], as_index=False)['cases'].sum()\n",
    "    df['reporting diff'] = df.groupby('code')['cases'].diff().convert_dtypes(convert_integer=True)\n",
    "    df = df[df['date reported'] == last_day]\n",
    "    df = df.sort_values(by=['reporting diff'], ascending=False)\n",
    "    \n",
    "    if code_group:\n",
    "        df = df[df.code.isin(code_group)]\n",
    "        df = df.groupby('date reported', as_index=False).sum()\n",
    "        \n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
